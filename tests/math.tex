% !TEX TS-program = arara
% arara: xelatex: { synctex: on, options: [-halt-on-error] } 
%% arara: biber
%% arara: makeglossaries
%% arara: makeindex
%% arara: xelatex: { synctex: on, options: [-interaction=batchmode, -halt-on-error] } 
%% arara: xelatex: { synctex: on, options: [-interaction=batchmode, -halt-on-error]  } 
% arara: clean: { extensions: [ aux, log, out, run.xml, toc, mw, synctex.gz, ] }
%% arara: clean: { extensions: [ bbl, bcf, blg, ] }
%% arara: clean: { extensions: [ glg, glo, gls, ] }
%% arara: clean: { extensions: [ idx, ilg, ind, xdy, ] }
%% arara: clean: { extensions: [ plCode, plData, plMath, plExercise, plNote, plQuote, ] }
%-----------------------------------------------------------------
\documentclass[12pt]{PalisadesLakesArticle}
\geomHDTV
\geomLandscape

%\AsanaFonts
%\CormorantFonts
%\EBGaramondFonts
\ErewhonFonts
%\LibertinusFonts
%\BonumFonts
%\PagellaFonts
%\ScholaFonts
%\TermesFonts
%\XITSFonts
%\STIXTwoFonts

%-----------------------------------------------------------------
\title{Font tests}
\author{John Alan McDonald 
(palisades dot lakes at gmail dot com)}
\date{draft of \today}
%-----------------------------------------------------------------
\begin{document}
%\maketitle
%\PalisadesLakesTableOfContents
%-----------------------------------------------------------------
\def\sharedFolder{../shared/}

\begin{plSection}{Expressions}

$
\frac{\partial f}{\partial x} 
\in \mathcal{F}\left(\symbb{R} \rightarrow \symbb{R}\right)$
;
$\nabla_{x} f (y) \in \codomain ( f )$

\begin{eqnarray}
\Vector{v_{01}} & = & \Point{p}_{0} - \Point{p}_{1} \in \symfrak{K}^n
\\
\Affine{A} \left( \Point{p} \right)
& = & 
\Linear{L}_{\Affine{A}} 
\left( 
\Point{p} - \Point{o}_{\Affine{A}}^{0}
 \right) 
+ 
\Point{o}_{\Affine{A}}^{1}
\\
\Linear{L}_{\Affine{A}} & : &\TangentSpace(\domain(\Affine{A}))
\rightarrow \TangentSpace(\codomain(\Affine{A}))
\\
\Point{o}_{\Affine{A}}^{0} & \in & \domain(\Affine{A})
\\
\Point{o}_{\Affine{A}}^{1} & \in & \codomain(\Affine{A})
\\
\Affine{A} \left( \Vector{v} \right)
& = & 
\Vector{L}_{\Affine{A}} \left( \Vector{v} \right)
+ \Vector{t}_{\Affine{A}} 
\\
\Derivative[\Vector{v}]{\Vector{f}}[\Vector{u}][\Vector{t}]
& = &
\Derivative[\Vector{v}_i]{\Vector{f}}
[(\Vector{u}_0 \ldots \Vector{u}_{n-1})][\Vector{t}_i] 
\\
\Partial{j}{\Vector{f}} & = &\Partial{v_j}{\Vector{f}} 
\\
\Derivative{\Vector{f}}[\Vector{u}]
& = &
\sum_{j=0}^{m-1} \Partial{j}{\Vector{f}}[\Vector{u}] 
\otimes \Vector{e}_j^{\Space{V}}
\\
\Partial{j}{\Vector{f}}[\Vector{u}]
& = &
\Derivative{\Vector{f}}[\Vector{u}] [\Vector{e}_j^{\Space{V}}]
\\
\Gradient{f}[\Vector{u}] 
& = &
\sum_{j=0}^{m-1} 
\left( 
\Partial{j}{f}[\Vector{u}] 
\right) \Vector{e}_j^{\Space{V}}
\\
\Derivative{\Vector{f}}[\Vector{u}]
& = &
 \sum_{i=0}^{n-1}  
\Vector{e}_i^{\Space{W}} 
\otimes \Gradient{\Vector{f}_i}[\Vector{u}]
\\
\tilde{\Vector{f}}  
& = &
\frac{\Vector{f}}{\| \Vector{f} \|}
\\
\Derivative{\tilde{\Vector{f}}}[\Vector{v}][\Vector{u}]
& = &
\frac{\Identity_{\Space{W}} 
\,-\, 
\left( 
\tilde{\Vector{f}}(\Vector{v})
 \otimes 
 \tilde{\Vector{f}}(\Vector{v}) \right)  
 }
{\| \Vector{f}(\Vector{v}) \|}
\Derivative{\Vector{f}}[\Vector{v}][\Vector{u}] 
\end{eqnarray}


See 
\citeAuthorYearTitle[chapter 2]{Spivak:1965:CalculusOnManifolds}.

%-----------------------------------------------------------------
\begin{plSection}{Spaces}
%-----------------------------------------------------------------
\begin{plSection}{Topological spaces}
\label{sec:Topological-spaces}

\begin{plDefinition}{\textsl{Continuity} of functions}{Continuity} 
$f \, : \, \Set{D} \rightarrow \Set{C}$,
for topological spaces $\Set{D}$ and $\Set{C}$,
is \textit{continuous}
if for any open set $\Set{O}_{\Set{C}} \subset \Set{C}$,
$f^{-1}\left( \Set{O}_{\Set{C}} \right) = 
\Set{O}_{\Set{D}}$, an open set $\subset \Set{D}$.

Example: open intervals in $\Space{R}$,
open balls in $\Space{R}^n$ with $l_1$, $l_2, l_{\infty}$ distances
(what is required to work?).
Figures for open balls with various metrics.
\end{plDefinition}%{\textit{Continuity} of functions}{Continuity} 

\end{plSection}%{Topological spaces}
%-----------------------------------------------------------------
\begin{plSection}{Linear spaces}
\label{sec:Linear-spaces}

\begin{plDefinition}{Linear space}{LinearSpace}
A \textit{linear space} 
$\Space{V} = \left[ \Set{V}, \Space{K}, \linearCombination \right]$
 is:
\begin{itemize}
  \item a set of \textit{vectors} $\Set{V}$,
  \item a field  of scalars $\Space{K}$,
  \item a linear combination function: 
\begin{equation}
\left( \linearCombination 
\, a_0 \, \Vector{v}_0 \, a_1 \, \Vector{v}_1 \right) \; 
 \mapsto \; \Vector{v}_2  \in \Set{V}
\end{equation}
for $\Vector{v}_0, \Vector{v}_1 \in \Set{V} $
and $a_0, a_1 \in \Space{K}$.
Linear combination is often defined in terms of
$2$ binary operations:
scalar multiplication $a * \Vector{v} \in \Set{V}$,
and vector addition $\Vector{v}_0 + \Vector{v}_1 \in \Set{V}$:
\begin{equation}
\left( \linearCombination 
\, a_0 \, \Vector{v}_0 \, a_1 \, \Vector{v}_1 \right) \; 
= \; a_0*\Vector{v}_0 + a_1*\Vector{v}_1
\end{equation}
\end{itemize}
\end{plDefinition}

Usually the distinction between $\Set{V}$ and $\Space{V}$ 
is ignored, and we will say, for example, 
$\Vector{v} \in \Space{V}$.

\begin{plDefinition}{Linear dependence}{LinearDependence}
Suppose $\Space{V}$ is a linear space and
$\Vector{v}_0, \ldots , \Vector{v}_{n-1} \in \Space{V}$.
If there exists $a_0, \ldots , a_{n-1}$ such that
$\Vector{0} = \sum a_i \Vector{v}_i$ then the $\left\{ \Vector{v}_i \right\}$
are \textit{linearly dependent}.
\citeAuthorYearTitle[section~5]{Halmos:1958:Finite}

Otherwise they are \textit{linearly independent}.
\end{plDefinition}

\begin{plTheorem}{The set of non-zero vectors
 $\Vector{v}_0, \ldots ,\Vector{v}_{n-1}$
is linearly dependent iff some $\Vector{v}_k, \; 1 \leq n-1$, 
is a linear combination of the preceding 
ones.}{linearDependenceTheorem}

See\citeAuthorYearTitle[Section 6]{Halmos:1958:Finite}.

\textsc{Proof:}
Assume  $\Vector{v}_0, \ldots ,\Vector{v}_{n-1}$ are linearly dependent.
Consider the smallest $k$ such that 
$\Vector{v}_0, \ldots ,\Vector{v}_{k}$ is linearly dependent.
By definition,
there exists non-zero $a_0, \ldots ,a_{k}$ such that
\begin{equation}
0 = \sum_0^k a_i \Vector{v}_i
\end{equation}
which implies that
\begin{equation}
\Vector{v}_k = \sum_0^{k-1} \frac{a_i}{- a_k} \Vector{v}_i
\end{equation}
\end{plTheorem}

Examples:

\begin{plExample}{$\Space{K}^n$}{}
%{}
%\bigskip
Where $\Space{K}$ is any field.
%\leavevmode \vspace{-\baselineskip}
\begin{itemize}
  \item vectors:
  $\Set{V} = \Space{K}^n = \left\{ \Vector{x}
  = \left[ x_0, \ldots , x_{n-1} \right] \right\}$,
  tuples of $n$ elements $x_i \in \Space{K}$.
  \item scalars: $\Space{K}$
  \item scalar multiplication:
  $ a *_{\Space{K}^n} \left[ x_0, \ldots , x_{n-1} \right] =
  \left[ \ldots , a *_{\Space{K}} x_{i} , \ldots \right]$
  \item vector addition:
  $\Vector{x} +_{\Space{K}^n} \Vector{y}
  = \left[ \ldots , \left( x_i +_{\Space{K}} y_i \right) ,\ldots \right]$
\end{itemize}
Includes $\Space{Q}$ and $\Space{R}$.
\end{plExample}

\begin{plExample}{The functions from any domain to some linear space.}{}
The functions from any domain to some linear space.
\textbf{TODO:} lisp notation for clarity below.
\begin{itemize}
  \item vectors: $\Set{F} = $ any function on $\Set{D}$
  that returns values in the linear space $\Space{V}$.
  \item scalars: $\Space{K}$, the same scalar field used by
  $\Space{V}=\left[ \Set{V}, \Space{K}, +, * \right]$.
  \item scalar multiplication:
  $ \left(a*\Vector{f}\right) : \Set{D} \rightarrow \Space{V}$
  is the function defined by
   $ \left(a*\Vector{f}\right) (x)
   = a*\left(\Vector{f}\left( x \right) \right) $
  \item vector addition:
  $\left( \Vector{f} + \Vector{g} \right) $
  is the function defined by
  $\left( \Vector{f} + \Vector{g} \right) \left( x \right) =
  \Vector{f} \left( x \right) + \Vector{g} \left( x\right)$
\end{itemize}
Canonical coordinates: 
$\Vector{f}\left( d \right) \; \forall d \in \Set{D}$
\end{plExample}

\begin{plExample}{{ $\Space{R}^n$ as a function space }}{}
We can identify 
(\citeAuthorYearTitle[sec.~24]{Halmos:1958:Finite}) 
$\Space{R}^n$ and 
$ \Space{F} \left[ \left\{ 0, 1, \ldots, {n-1} \right\}, \Space{R} \right] $
by
$\Vector{x} \Leftrightarrow f_{\Vector{x}}$
where $f_{\Vector{x}} \left( i \right) = \Vector{x}_i$
for $\Vector{x} \in \Space{R}^n$ and 
$i \in \left\{ 0, 1, \ldots, {n-1} \right\}$
\end{plExample}

%-----------------------------------------------------------------
\begin{plSection}{Inner product (linear) spaces}
Let $\Space{V}$ be an $n$-dimensional real inner product space.
Let $\Vector{v}, \Vector{w} \in \Space{V}$.

\begin{itemize}
\item The inner (dot) product on $\Reals^n$:
\begin{equation}
\Vector{v} \bullet \Vector{w} \; \equiv \; \sum_{i=0}^{n-1} v_i w_i
\end{equation}

\item The euclidean ($l_2$) norm:
\begin{equation}
\| \Vector{v} \|^2 \; \equiv \; \Vector{v} \bullet \Vector{v}
\end{equation}

\item $\theta(\Vector{v},\Vector{w})$ is the angle between $\Vector{v}$ and $\Vector{w}$
and is defined by:
\begin{eqnarray}
\Vector{v} \bullet \Vector{w} \; = \; \| \Vector{v} \| \| \Vector{w} \| \cos(\theta(\Vector{v},\Vector{w}))
\\
\theta(\Vector{v},\Vector{w})
\; \equiv \;
\cos^{-1} \left(\frac{ \Vector{v} \bullet \Vector{w} }{\| \Vector{v} \| \| \Vector{w} \| } \right)
\nonumber
\end{eqnarray}

\item The tensor (outer) product:

Let $\Vector{v}, \Vector{u} \in \Space{V}, \Vector{w} \in \Space{W}.$
$\Vector{w} \otimes \Vector{v}$ is a rank 1 linear map
from $\Space{V}$ to $\Space{W}$, defined by:
\begin{equation}
(\Vector{w} \otimes \Vector{v})
(\Vector{u}) \; \equiv \; \Vector{w} 
(\Vector{v} \bullet \Vector{u})
\end{equation}

Note: this is an abuse of the usual definition of tensor product $\otimes$.
This operation, which takes a pair of vectors and returns a linear map,
is more conventionally referred to as the 'outer product',
and written $\Vector{w} \Vector{v}^{\dagger}$.
However, because I am working in spaces other than $\Reals^n$
(eg. $\Space{L}(\Space{V},\Space{W})$, the space of linear maps
between 2 vector spaces),
I want to avoid notations that suggest thinking in terms
of 'row' and 'column' vectors.

The following is a useful identity.
If $\Vector{t} \in \Space{T}$, 
$\Vector{u}, \Vector{v} \in \Space{V}$, 
and $\Vector{w} \in \Space{W}.$
then
\begin{equation}
\label{eq:tensor-dot}
(\Vector{t} \otimes \Vector{u}) 
(\Vector{v} \otimes \Vector{w})(\Vector{u}) 
= (\Vector{u} \bullet \Vector{v}) (\Vector{t} \otimes \Vector{w})
\end{equation}

\item Elementary orthogonal projection:
\begin{equation}
\Projection_{\Vector{w}} \Vector{v}
\; \equiv \;
\left( \frac{ \Vector{w} }{ \| \Vector{w} \| } \otimes 
\frac{ \Vector{w} }{ \| \Vector{w} \| } \right) \Vector{v}
\; = \;
\left(
\frac{\Vector{w} }{\|\Vector{w}\|} \bullet \Vector{v} 
\right) 
\frac{\Vector{w}}{\|\Vector{w}\|}
\end{equation}

\item Orthogonal complement:
\begin{equation}
\perp_{\Vector{w}} \Vector{v}
\; \equiv \;
\Vector{v} \perp \Vector{w}
\; \equiv \;
\Vector{v} \; - \; \Projection_{\Vector{w}} \Vector{v}
\; = \;
\Vector{v} \; - \; 
\left( 
\frac{\Vector{w}}{\|\Vector{w}\|} \bullet \Vector{v} 
\right) 
\frac{\Vector{w}}{\|\Vector{w}\|}
\end{equation}

\end{itemize}
\end{plSection}%{Inner product (linear) spaces}
%-----------------------------------------------------------------
\end{plSection}%{Linear spaces}
%-----------------------------------------------------------------
\begin{plSection}{Functions between spaces}
\label{sec:Functions-between-spaces}

In general, the functions discussed here map between real inner product spaces:
$\Vector{f}:\Space{V} \mapsto \Space{W}$, where $\Space{V}$ is the
\textit{domain} and $\Space{W}$ is the \textit{codomain}.
The real inner product spaces are almost derived from some $\Reals^n$.

The \textit{range} of 
$\Vector{f}$, $\range(\Vector{f})$, is the set $\Vector{f}(\Space{V})$,
which may be a proper subset of its codomain $\Space{W}$.
The \textit{kernel} of
 $\Vector{f}$, $\kernel(\Vector{f})$, is the set
$\kernel(\Vector{f}) = \left\{ \Vector{v} \in \Space{V}
 : \Vector{f}(\Vector{v}) = \Vector{0} \right\}$.

When I want to distinguish between real- and vector-valued functions,
I may use 'function' for vector-valued functions and
'functional' for real-valued ones.

I use $\Space{U}$, $\Space{V}$, $\Space{W}$ for generic linear spaces,
$\Vector{u}$, $\Vector{v}$, $\Vector{w}$, etc., for elements of linear spaces,
usually called \textit{vectors}
and
$\Vector{f}$, $\Vector{g}$, $\Vector{h}$ for vector-valued functions.
I generally do not distinguish $\Reals$, the real numbers,
and $\Reals^1$, or any other 1-dimensional real linear space.
I sometimes use $f$, $g$, $h$ for extra clarity in the special
case of real-valued functions.

The domains of many interesting functions,
such as those that depend on vertex positions,
are direct sum of inner product spaces.
The \textit{direct sum} $\Space{V} \oplus \Space{W}$ is the inner product space
consisting of the ordered pairs 
$\left\{ (\Vector{v},\Vector{w}) : 
\Vector{v} \in \Space{V}, \Vector{w} \in \Space{W} \right\}$
inheriting the inner product space operations in the obvious way:
$(\Vector{v}_0,\Vector{w}_0) \bullet (\Vector{v}_1,\Vector{w}_1) 
= (\Vector{v}_0 \bullet \Vector{v}_1) 
+ (\Vector{w}_0 \bullet \Vector{w}_1).$
I will usually write an element of $\oplus^n \Space{V}$ as
$(\Vector{v}_0,\ldots,\Vector{v}_{n-1})$
and use
$\Vector{f}(\Vector{v}_0,\Vector{v}_1,\ldots,\Vector{v}_{n-1})$
for a function that depends on $n$ vectors.

%-----------------------------------------------------------------
\begin{plSection}{Linear functions}
\label{sec:linear-functions}

A function $\Vector{L}(\Vector{v}):\Space{V} \mapsto \Space{W}$
is \textit{linear} iff
$\Vector{L}(a_0 \Vector{v}_0 + a_1 \Vector{v}_1) 
= a_0 \Vector{L}(\Vector{v}_0) + a_1 \Vector{L}(\Vector{v}_1)$.
I will often write
 $\Vector{L}\Vector{v} \equiv \Vector{L}(\Vector{v})$.

It is not hard to see that, for a linear function,
the range and kernel are linear subspaces of the codomain and
domain, respectively.
Thus any linear function between inner product spaces
divides its domain and codomain each into 2 orthogonal subspaces.
The domain is divided into 
$\Space{V} = \kernel(\Vector{L}) \oplus \kernel^{\perp}(\Vector{L})$,
and the codomain is divided into 
$\Space{W} = \range(\Vector{L}) \oplus \range^{\perp}(\Vector{L})$.

The most common representation for linear functions is the \textit{matrix:}
Let $\Vector{L}(\Vector{v}):\Space{V} \mapsto \Space{W}$ be linear,
$\left\{ \Vector{e}_0^{\Space{V}} \ldots  \Vector{e}_{m-1}^{\Space{V}} \right\}$ 
an orthonormal basis for $\Space{V}$,
and
$\left\{ \Vector{e}_0^{\Space{W}} \ldots \Vector{e}_{n-1}^{\Space{W}} \right\}$ 
an orthonormal  basis for $\Space{W}$
Then $\Vector{L}$ can be expressed as
\begin{equation}
\Vector{L}
 =
\sum_{i=0}^{m-1} \sum_{j=0}^{n-1} L_{ij}
 ( \Vector{e}_i^{\Space{W}} \otimes \Vector{e}_j^{\Space{V}} )
\end{equation}
$(L_{ij})$ is the matrix representation of $\Vector{L}$ with respect to
the two bases \citeAuthorYearTitle{Halmos:1958:Finite}.

Examples:

\begin{itemize}

\item Column-wise:
$\Vector{L} = \sum_{j=0}^{n-1} 
( \Vector{c}_j^{\Vector{L}} \otimes \Vector{e}_j^{\Space{V}} )$

$\Vector{c}_j^{\Vector{L}} \in \Space{W}$ 
are the 'columns' of $\Vector{L}$.
$\linearspan\left\{ \Vector{c}_0^{\Vector{L}} 
\ldots \Vector{c}_{n-1}^{\Vector{L}} \right\} = \range(\Vector{L})$
(see \cref{sec:spans-and-projections}).

\item Row-wise:
$\Vector{L} = \sum_{i=0}^{m-1} ( \Vector{e}_i^{\Space{W}} \otimes  \Vector{r}_i^{\Vector{L}} )$

$\Vector{r}_i^{\Vector{L}} \in \Space{V}$ are the 'rows' of $\Vector{L}$.
$\linearspan\left\{ \Vector{r}_0^{\Vector{L}} \ldots \Vector{r}_{m-1}^{\Vector{L}} \right\} =  \kernel(\Vector{L})^{\perp}$
(see \cref{sec:spans-and-projections}).

\item Householder:
$\Vector{h}_{\Vector{v}} 
= \Identity_{\Space{V}} - \frac{2}{\| \Vector{v} \|^2} 
(\Vector{v} \otimes \Vector{v})$

Householder maps are usually chosen to zero the elements of
a vector, or a row or column of a matrix, for a contiguous range of
indices, say, $[i_0,\ldots,i_n)$.

\end {itemize}

\end{plSection}%{Linear functions}
%-----------------------------------------------------------------
\begin{plSection}{Affine functions}
\label{sec:affine-functions}

A function $\Point{A}(\Vector{v}):\Space{V} \mapsto \Space{W}$
is \textit{affine} if distributes over affine combinations:
$\Point{A}(\sum_{i=0}^{n-1} 
a_i \Vector{v}_i) = \sum_{i=0}^{n-1} a_i \Point{A}(\Vector{v}_i) $
for all $\left\{a_i\right\}$ such that $1 = \sum_{i=0}^{n-1} a_i$.
(Note that I am describing affine functions on vector (linear) spaces,
rather than the slightly more general notion of affine functions on affine spaces.)
Any linear function between linear spaces is automatically affine.
The other major class of affine functions on linear spaces are the translations.
A \textit{translation,} $\Point{T}_{\Vector{t}}$, $\Space{V} \mapsto \Space{V}$,
simply adds a vector ($\Vector{t}$) to its argument:
$\Point{T}_{\Vector{t}} \Vector{v} = \Vector{v} + \Vector{t}$.
It's not hard to see that any affine function between two linear spaces
can be represented as the sum of a linear function and a translation.
A typical representation for a general affine function 
$\Point{A} : \Space{V} \mapsto \Space{W}$
is as a pair $(\Vector{L},\Vector{t})$ where $
\Vector{L} : \Space{V} \mapsto \Space{W}$ is linear,
$\Vector{t} \in \Space{W}$, and 
$\Point{A}(\Vector{v}) = \Vector{L}(\Vector{v}) + \Vector{t}$.

\end{plSection}%{Affine functions}
%-----------------------------------------------------------------
\begin{plSection}{Spans and projections}
\label{sec:spans-and-projections}

Let $\Space{V}$ be an $n$-dimensional inner product space.

The \textit{linear span} of a set of $m$ vectors in $\Space{V}$
is the set of linear combinations of those vectors:
\begin{equation}
\linearspan\left\{ \Vector{v}_0 \ldots \Vector{v}_{m-1} \right\} = \left\{\Vector{v} \in \Space{V} : \Vector{v} = \sum_{i=0}^{m-1} a_i \Vector{v}_i\right\}
\end{equation}
$\linearspan\left\{ \Vector{v}_0 \ldots \Vector{v}_{m-1} \right\}$ is a linear subspace of $\Space{V}$.

The \textit{projection} $\Projection_{\Set{S}} \Vector{v}$ of a vector $\Vector{v} \in \Space{V}$
onto an arbitrary subset $\Set{S} \subset \Space{V}$
is the closest point in $\Set{S}$ to $\Vector{v}$.
Projection onto a linear subspace is a linear function and
can be computed by summing
elementary orthogonal projections onto an orthonormal basis for the subspace.

An orthonormal basis for $\linearspan\left\{ \Vector{v}_0 \ldots \Vector{v}_{m-1} \right\}$
(and $\linearspan\left\{ \Vector{v}_0 \ldots \Vector{v}_{m-1} \right\}^\perp$)
can be computed using the QR decomposition
of the function $\Vector{V} = \sum_{i=0}^{m-1} \Vector{v}_i \otimes \Vector{e}_i$,
(the $n \times m$ matrix whose columns are the $\Vector{v}_i$)
(see \citeAuthorYearTitle[sec.~5.2 ]{GolubVanLoan:2012}).

The \textit{affine span} of a set of $m+1$ vectors in $\Space{V}$
is the set of affine combinations of those vectors:
\begin{equation}
\affinespan\left\{ \Point{p}_0 \ldots \Point{p}_{m} \right\} = \left\{\Vector{v} \in \Space{V} : \Vector{v} = \sum_{i=0}^{m} b_i \Point{p}_i;
1 = \sum_{i=0}^{m} b_i \right\}.
\end{equation}
$\affinespan\left\{ \Point{p}_0 \ldots \Point{p}_{m} \right\}$ is an affine subspace of $\Space{V}$.
$\Vector{b} = ( b_0 \ldots b_m )$ are \textit{barycentric coordinates}
for $\Vector{v}$ with respect to $\left\{ \Point{p}_0 \ldots \Point{p}_{m} \right\}$.
The barycentric coordinates are unique if $\left\{ \Point{p}_0 \ldots \Point{p}_{m} \right\}$
are affinely independent.

Any affine subspace, $\Space{A}$, of a linear space, $\Space{V}$ can be represented as
as a translation of a linear subspace of $\Space{V}$:
$\Space{A} = \Space{T}(\Space{A}) + \Vector{t}$,
$\Space{T}(\Space{A})$ is the set of differences of elements of $\Space{A}$,
a linear subspace of $\Space{V}$.
If $\Vector{t}$ is any element of $\Space{A}$.
then projection onto $\Space{A}$
can be computed as a translation of an orthogonal projection onto $\Space{T}(\Space{A})$:
$\Projection_{\Space{A}} (\Point{p}) = \Vector{t} + \Projection_{\Space{T}(\Space{A})} (\Point{p} - \Vector{t})$.
Typically, we pick $\Vector{t}$ to be the smallest element of $\Space{A}$.
Projection onto an affine space is clearly an affine function.

We can represent the affine span of a set of $m+1$ vectors
as a translation of a linear span:
\begin{equation}
\affinespan\left\{ \Point{p}_0 \ldots \Point{p}_{m} \right\} = \Point{p}_m + \linearspan\left\{\Vector{v}_0 \ldots \Vector{v}_{m-1}\right\}
\end{equation}
where $\Vector{v}_i = \Point{p}_i - \Point{p}_m$,
which allows us to compute the projection onto
$\affinespan\left\{ \Point{p}_0 \ldots \Point{p}_{m} \right\}$
again using the QR decomposition
of $\Vector{V} = \sum_{i=0}^{m-1} \Vector{v}_i \otimes \Vector{e}_i$.

\end{plSection}%{Spans and projections}
%-----------------------------------------------------------------
\begin{plSection}{Linear inverses and pseudo-inverses}
\label{sec:Linear-inverses-and-pseudo-inverses}

A convenient definition for the \textit{true inverse}
of a function $\Vector{f}(\Vector{v}):\Space{V} \mapsto \Space{W}$ is
$\Vector{f}^{-1}(\Vector{w}) = \left\{ \Vector{v} : \Vector{f}(\Vector{v}) = \Vector{w} \right\}$.
The usual definition of inverse treats $\Vector{f}^{-1}$
as a function from $\Space{W} \mapsto \Space{V}$,
which is undefined where the value of the true
inverse is not a set containing a single point.

For functions between inner product spaces,
the \textit{pseudo-inverse}, $f^{-}$, is a function $\Space{W} \mapsto \Space{V}$
defined everywhere on $\Space{W}$.
Let $\hat{\Vector{w}}$ be an element of $\Space{W}$ closest to $\Vector{w}$
such that $\Vector{f}^{-1}(\Vector{w})$ is not empty.
Let $\hat{\Vector{v}}$ be a minimum norm element of $\Vector{f}^{-1}(\hat{\Vector{w}})$.
Then $\Vector{f}^{-}(\Vector{w}) = \hat{\Vector{v}}$.

If $\Vector{L}$ is linear, then it's not hard to see that
$\hat{\Vector{w}} = \pi_{\range(\Vector{L})} \Vector{w}$, the projection of $\Vector{w}$
on the range of $\Vector{L}$
and
$\hat{\Vector{v}}$ is the unique element of $\kernel^{\perp}(\Vector{L})$
such that $\Vector{L}(\hat{\Vector{v}}) = \hat{\Vector{w}}$.

The pseudo-inverse of a linear function can be characterized
by the four Moore-Penrose 
conditions \citeAuthorYearTitle[sec.~5.5.2]{GolubVanLoan:2012}:
\begin{enumerate}
\item $\Vector{L} \Vector{L}^{-} \Vector{L} = \Vector{L}$
\item $\Vector{L}^{-} \Vector{L} \Vector{L}^{-} = \Vector{L}^{-}$
\item $\left( \Vector{L} \Vector{L}^{-} \right)^{\dagger} = \Vector{L} \Vector{L}^{-}$
\item $\left( \Vector{L}^{-} \Vector{L} \right)^{\dagger} = \Vector{L}^{-} \Vector{L}$
\end{enumerate}

When the 'columns' of $\Vector{L}$, $\Vector{r}_j^{\Vector{L}}$
($\Vector{L} = \sum_{j=0}^{n-1} ( \Vector{L}_j^{\Space{W}} \otimes \Vector{e}_j^{\Space{V}} )$)
are linearly independent,
then a useful identity is:
\begin{equation}
\label{eq:full-rank-pseudo-inverse}
\Vector{L}^{-} = \left( \Vector{L}^{\dagger} \Vector{L} \right)^{-1} \Vector{L}^{\dagger}
\end{equation}

The pseudoinverse can be computed
using standard matrix decompositions such as
the QR and SVD \citeAuthorYearTitle{GolubVanLoan:2012}.
The pseudoinverse is an example of a linear transformation
which should {\em not} be represented by a matrix
\citeAuthorYearTitle{McDonald:1989:OOPSLA}.

If $\Point{A}$ is affine,
let $\Point{A} = \Vector{L} + \Vector{t}$,
where $\Vector{L}$ is linear,
and $\Vector{t}$ is an element of $\range(\Point{A})$.
Then $\Point{A}^{-}(\Vector{w}) = \Vector{L}^{-}( \Vector{w} - \Vector{t} )$.
\end{plSection}%{Linear inverses and pseudo-inverses}
%-----------------------------------------------------------------
\end{plSection}%{Functions between spaces}
%-----------------------------------------------------------------
\end{plSection}%{Spaces}
%-----------------------------------------------------------------
\end{plSection}%{Fonts}
%\import{\sharedFolder}{derivatives}
%\import{\sharedFolder}{category}
%-----------------------------------------------------------------
\end{document}
%-----------------------------------------------------------------
